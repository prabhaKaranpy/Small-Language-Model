{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84b3fc3-cfa1-4fc1-b2c0-42fa281ab8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3050 A Laptop GPU\n",
      "Using device: cuda\n",
      "\n",
      "Tensor on GPU:\n",
      "tensor([[-0.5023,  0.2382],\n",
      "        [ 2.8655, -2.2159]], device='cuda:0')\n",
      "Tensor device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    # Print number of GPUs\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    # Print the name of the first GPU\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Set the device to GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Perform a simple tensor operation on the GPU\n",
    "    a = torch.randn(2, 3, device=device) # Create a tensor on the GPU\n",
    "    b = torch.randn(3, 2, device=device) # Create another tensor on the GPU\n",
    "    c = torch.matmul(a, b)             # Perform matrix multiplication on the GPU\n",
    "    print(\"\\nTensor on GPU:\")\n",
    "    print(c)\n",
    "    print(f\"Tensor device: {c.device}\") # Verify it's on the GPU\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will run on CPU.\")\n",
    "    # Perform a simple tensor operation on the CPU (if no GPU)\n",
    "    a = torch.randn(2, 3)\n",
    "    b = torch.randn(3, 2)\n",
    "    c = torch.matmul(a, b)\n",
    "    print(\"\\nTensor on CPU:\")\n",
    "    print(c)\n",
    "    print(f\"Tensor device: {c.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c57cf62-d2c5-4528-87af-7d2f6a6e9ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.32.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18226b4-194c-4699-8e3a-6306e9c4c75a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since roneneldan/TinyStories couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\HP\\.cache\\huggingface\\datasets\\roneneldan___tiny_stories\\default\\0.0.0\\f54c09fd23315a6f9c86f9dc80f725de7d8f9c64 (last modified on Sat Jun 14 10:44:15 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n",
      "OutSide :  <Encoding 'gpt2'>\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "import tiktoken\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"roneneldan/TinyStories\") \n",
    "print(ds)\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"OutSide : \", enc); \n",
    "# print(enc.encode_ordinary(ds));\n",
    "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n",
    "\n",
    "def process(example):\n",
    "    # print(\"Hello, World!\"); (after execution, I commented this line); \n",
    "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=1,\n",
    "        )\n",
    "    # concatenate all the ids in each dataset into one large file we can use for training\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            # Batch together samples for faster write\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            # Write into mmap\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34b32ea-2be2-46ea-910c-708c3c08db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be30512-0621-4d9a-bc0a-575855632dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ce38c77-1b1b-4656-ab7e-142b5795858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,\n",        # no. of transformers
    "    n_head=6,\n",         # no. of attention heads 
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf63209-dcc4-43f6-a80a-aa71ed90dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a0a5e3-c75b-4cfa-835b-3435fae14fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19198957850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Config\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
    "max_iters = 30000 #increase from 25000\n",
    "warmup_steps = 1000 #smoother initial train, earlier 100\n",
    "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
    "eval_iters = 500 # increased from 100\n",
    "batch_size = 32 # changed from 16, better gradient estimate\n",
    "block_size = 128 #changed from 64, capture longer range dependencies\n",
    "\n",
    "gradient_accumulation_steps = 32 # reduced from 50\n",
    "\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "\n",
    "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
    "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d79097-4000-4419-960a-0e06016a84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
    "\n",
    "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
    "\n",
    "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a8e1a06-7403-4144-bc07-23f3defe23a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb3d037ae994bd19307c269253b3d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: train loss 9.4801, val loss 9.4852\n",
      "The current learning rate: 0.00007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: train loss 8.5219, val loss 8.5252\n",
      "The current learning rate: 0.00010\n",
      "Epoch 1500: train loss 7.5736, val loss 7.5762\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2000: train loss 6.7314, val loss 6.7309\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2500: train loss 6.0429, val loss 6.0386\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3000: train loss 5.5399, val loss 5.5365\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3500: train loss 5.1306, val loss 5.1258\n",
      "The current learning rate: 0.00011\n",
      "Epoch 4000: train loss 4.8170, val loss 4.8087\n",
      "The current learning rate: 0.00011\n",
      "Epoch 4500: train loss 4.5754, val loss 4.5760\n",
      "The current learning rate: 0.00011\n",
      "Epoch 5000: train loss 4.3737, val loss 4.3716\n",
      "The current learning rate: 0.00012\n",
      "Epoch 5500: train loss 4.2168, val loss 4.2093\n",
      "The current learning rate: 0.00012\n",
      "Epoch 6000: train loss 4.0536, val loss 4.0549\n",
      "The current learning rate: 0.00013\n",
      "Epoch 6500: train loss 3.9238, val loss 3.9270\n",
      "The current learning rate: 0.00013\n",
      "Epoch 7000: train loss 3.8127, val loss 3.8109\n",
      "The current learning rate: 0.00014\n",
      "Epoch 7500: train loss 3.7137, val loss 3.7105\n",
      "The current learning rate: 0.00015\n",
      "Epoch 8000: train loss 3.6076, val loss 3.6148\n",
      "The current learning rate: 0.00015\n",
      "Epoch 8500: train loss 3.5270, val loss 3.5261\n",
      "The current learning rate: 0.00016\n",
      "Epoch 9000: train loss 3.4480, val loss 3.4535\n",
      "The current learning rate: 0.00017\n",
      "Epoch 9500: train loss 3.3838, val loss 3.3741\n",
      "The current learning rate: 0.00018\n",
      "Epoch 10000: train loss 3.3097, val loss 3.3183\n",
      "The current learning rate: 0.00019\n",
      "Epoch 10500: train loss 3.2457, val loss 3.2517\n",
      "The current learning rate: 0.00020\n",
      "Epoch 11000: train loss 3.1893, val loss 3.1968\n",
      "The current learning rate: 0.00021\n",
      "Epoch 11500: train loss 3.1332, val loss 3.1327\n",
      "The current learning rate: 0.00022\n",
      "Epoch 12000: train loss 3.0762, val loss 3.0848\n",
      "The current learning rate: 0.00023\n",
      "Epoch 12500: train loss 3.0289, val loss 3.0312\n",
      "The current learning rate: 0.00024\n",
      "Epoch 13000: train loss 2.9819, val loss 2.9895\n",
      "The current learning rate: 0.00025\n",
      "Epoch 13500: train loss 2.9401, val loss 2.9462\n",
      "The current learning rate: 0.00026\n",
      "Epoch 14000: train loss 2.8914, val loss 2.8977\n",
      "The current learning rate: 0.00027\n",
      "Epoch 14500: train loss 2.8589, val loss 2.8627\n",
      "The current learning rate: 0.00028\n",
      "Epoch 15000: train loss 2.8255, val loss 2.8225\n",
      "The current learning rate: 0.00029\n",
      "Epoch 15500: train loss 2.7824, val loss 2.7835\n",
      "The current learning rate: 0.00030\n",
      "Epoch 16000: train loss 2.7435, val loss 2.7455\n",
      "The current learning rate: 0.00031\n",
      "Epoch 16500: train loss 2.7046, val loss 2.7117\n",
      "The current learning rate: 0.00032\n",
      "Epoch 17000: train loss 2.6750, val loss 2.6854\n",
      "The current learning rate: 0.00033\n",
      "Epoch 17500: train loss 2.6454, val loss 2.6501\n",
      "The current learning rate: 0.00034\n",
      "Epoch 18000: train loss 2.6201, val loss 2.6225\n",
      "The current learning rate: 0.00035\n",
      "Epoch 18500: train loss 2.5892, val loss 2.5915\n",
      "The current learning rate: 0.00036\n",
      "Epoch 19000: train loss 2.5623, val loss 2.5626\n",
      "The current learning rate: 0.00037\n",
      "Epoch 19500: train loss 2.5372, val loss 2.5351\n",
      "The current learning rate: 0.00038\n",
      "Epoch 20000: train loss 2.5117, val loss 2.5084\n",
      "The current learning rate: 0.00039\n",
      "Epoch 20500: train loss 2.4874, val loss 2.4941\n",
      "The current learning rate: 0.00040\n",
      "Epoch 21000: train loss 2.4643, val loss 2.4605\n",
      "The current learning rate: 0.00041\n",
      "Epoch 21500: train loss 2.4425, val loss 2.4411\n",
      "The current learning rate: 0.00042\n",
      "Epoch 22000: train loss 2.4134, val loss 2.4192\n",
      "The current learning rate: 0.00043\n",
      "Epoch 22500: train loss 2.3952, val loss 2.3955\n",
      "The current learning rate: 0.00044\n",
      "Epoch 23000: train loss 2.3844, val loss 2.3811\n",
      "The current learning rate: 0.00045\n",
      "Epoch 23500: train loss 2.3500, val loss 2.3538\n",
      "The current learning rate: 0.00045\n",
      "Epoch 24000: train loss 2.3372, val loss 2.3400\n",
      "The current learning rate: 0.00046\n",
      "Epoch 24500: train loss 2.3254, val loss 2.3181\n",
      "The current learning rate: 0.00047\n",
      "Epoch 25000: train loss 2.2957, val loss 2.3064\n",
      "The current learning rate: 0.00047\n",
      "Epoch 25500: train loss 2.2823, val loss 2.2931\n",
      "The current learning rate: 0.00048\n",
      "Epoch 26000: train loss 2.2697, val loss 2.2685\n",
      "The current learning rate: 0.00048\n",
      "Epoch 26500: train loss 2.2568, val loss 2.2672\n",
      "The current learning rate: 0.00049\n",
      "Epoch 27000: train loss 2.2391, val loss 2.2463\n",
      "The current learning rate: 0.00049\n",
      "Epoch 27500: train loss 2.2289, val loss 2.2320\n",
      "The current learning rate: 0.00049\n",
      "Epoch 28000: train loss 2.2149, val loss 2.2135\n",
      "The current learning rate: 0.00050\n",
      "Epoch 28500: train loss 2.2131, val loss 2.2163\n",
      "The current learning rate: 0.00050\n",
      "Epoch 29000: train loss 2.1853, val loss 2.1857\n",
      "The current learning rate: 0.00050\n",
      "Epoch 29500: train loss 2.1805, val loss 2.1821\n",
      "The current learning rate: 0.00050\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# In your training loop\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        # Ensure estimate_loss uses the correct device\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list += [losses['train']]\n",
    "        validation_loss_list += [losses['val']]\n",
    "\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    # Ensure X and y are on the correct device\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71d394f7-1525-43d5-baad-b72b62f21e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\pytorch_gpu_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b30527e-1e04-4dcc-91e9-f99ee6c39181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
    "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
    "plt.xlabel(\"Steps - Every 100 epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ed64e28-9c09-422a-8ca6-4fb553b70a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16140\\1220266581.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model\n",
    "model = GPT(config)  # re-create the model with same config\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "effd15b6-abcb-4b30-bb40-5357ce391e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a pumpkin. It was big and happy to eat some money. One day, something special happened. Out, it got lost! The volcano was stuck in a fence! The pumpkin was not interesting. It could not move.\n",
      "\n",
      "Lucy thought for a moment and then decided to sign what it would be for very long. She stepped onto the fence and asked the bull what it had to do. The bull was so excited!\n",
      "\n",
      "The seal quickly opened the gate and touched the pumpkin. Inside it reached the cupboard, horror. Lucy's mom had an idea! The bull decided it was an adventure.\n",
      "\n",
      "When the bean arrived at her room, she saw a tall castle that alas, and some broken color of wood. She could not breathe! She waited patiently, but theara was very scared and ran off to find her mom, who big smile had an idea.\n",
      "\n",
      "The moral of the story is that you don't need a mind, well before touching your homes!Once upon\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Once upon a time there was a pumpkin.\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4802a105-8bd7-4959-9ada-5793b31a4eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A beautiful girl named Lucy. She lived some toys and mind eating. One day, Lucy made an itch and tried to make it freeze. To her surprise, it was a surprise!\n",
      "\n",
      "\"Oh of the spider!\" she said excitedly.\n",
      "\n",
      "Her friend had an idea. \"Let's go explore the fog doesn't like the same way darkness. We need to wait someone to wait in our adventure!\"\n",
      "\n",
      "Lucy stayed still, until her mom turned out and saw a rude monster. It seemed like her mom, share with her. Lucy smiled and kept exploring.\n",
      "\n",
      "The earthquake was filled with strong importantly life, neverplease guys!Once upon a time, there was a little girl. Her name was Lizzy. She loved to play outside and play. One day, Lizzy found a thankful naughty butterfly. \n",
      " When her kitten woke up, she lifted her hand and held it up. Suddenly, the butterfly flew away. The bee had gotten so useful that she wanted to\n"
     ]
    }
   ],
   "source": [
    "sentence = \"A beautiful girl\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98ea6af3-5af1-45c4-9758-943fdd3ab5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sunny afternoon!\"Once upon a time there was a smooth mustache. It was shining underground. It had a big belt that could be stuck in it! One day, the strong scale decided to take a walk in the woods. As the giant was walking, it saw a small task - it had never seen it before. It was really rusty. The biggest gun was just then it caught. \n",
      "Suddenly, there was a squirrel standing out of the woods. The squirrel was walking up and scary at something it wasn't stop. It was an animal that was missing its pretty brother. \n",
      "The two them both got very angry and asked the other cat a wise one if it was refused. The other balls smiled sadly at them and said, \"I know where you're, invite you what we don't have!\" \n",
      "\n",
      "The second friend said, \"That's law of age - don't you let us get hurt even more electricity and the hurry up?\"\n",
      "\n",
      "The little puppies nodded and promised\n"
     ]
    }
   ],
   "source": [
    "sentence = \"A sunny afternoon\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd044ecf-e8b5-4207-aaa7-a2d79994837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy went to ground. The object was filled with all of power andfulness. He had plucked the underground, so that engines rolled very well.\n",
      "\n",
      "The boy was so excited that he alert about to do something special. He ran outside and swtours the tubes all around the same. As he got into the rock, the boy never bumped into the leaves! He learnt a valuable lesson that day: no matter what, something is the future and you must listen great rules.Once there was a girl named Molly who loved to wear super speed around. She had just Mother's comfort that they knew to do something new today. \n",
      "\n",
      "One day, she was running around her room. She decided to act and sit in a neat drawer. All her planes around her Sarah would feel that she would again soon wait.\n",
      "\n",
      "The end.Once upon a time there was a tap. It was a big push shape. It was a big and strong earthquake was moving in a tree. Its wind was\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The boy went to ground\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdddfbac-fd98-4966-8842-f54ccf38b3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a natural scenario until she gave him cat a magic wish and just enjoyed the messages. Ned still realized that being brave and hungry is the best little one.\n",
      "\n",
      "Every day Henry and Rover were finished together and the day he never screamed again. He kept walking and Jerey followed Earth around the field. When he noticed a bright rainbow.\n",
      "\n",
      "Inside, fly was a beautiful butterfly bloomed with beautiful wings and sparkly! It was incredible and amazing. He was so glad that he had imagined and life in that he can anyone else have them.\n",
      "\n",
      "And that is when creatures ever himself longer planned.Once there was a little girl and a witch. She was very excited and liked to hide along. She found lots of magic to help her family when she found them.\n",
      "\n",
      "One day, the witch saw a big tree in the other living room. She was delighted to start all the recordings.\n",
      "\n",
      "She had never been ready to find the magical place before. She followed it out of the\n"
     ]
    }
   ],
   "source": [
    "sentence = \"a natural scenario\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9749f17-2cbb-47e0-9ffd-ede36f42d30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Philosopher's Stone lovely petals made his nest comfortling nicebye and laughter for it! He was and happy that his garden was rewarded when he was full. \n",
      "\n",
      "Tenny was extra curious about the sheep. He had never seen such a beautiful wool before and he couldn't wait to feed it before he waved goodbye to the stars. He knew he had been more important, even about that lively world settled to the kitchen.Once upon a time there was a gray model knob. Then one day it started to move away. It got wide and couldn't wait to start the other value! It was so happy, and inside was so excited and, it knew that when it had been gone through the village.\n",
      "\n",
      "Soon a man came to the ordinary town. The special day smiled and asked the other kids what they had done. The gloomy day he he apologised for it.\n",
      "\n",
      "The two kids became brave and determined to hide and seek. Everyone stood very wealthy, where they would only stayed all night to reach until they all agreed. After a few days, they would find the monster just could stay forever. Everyone was so pleased and grateful to have a better explore often - nobody was showing off the results and to the problem.Once upon a time there was a lonely bear. He liked to solve fire with his friends. One day he found a letter and tried to guess it about something nice to him.\n",
      "\n",
      "As he opened his bag he saw a tree flying in the sky. He\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Harry Potter and the Philosopher's Stone\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 300)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57787125-6899-410f-a2bd-056d5dcb7f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch GPU)",
   "language": "python",
   "name": "pytorch_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
